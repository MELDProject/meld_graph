{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook to plot example reports \n",
    "\n",
    "plot prediction, saliencies and MRI \n",
    "\n",
    "need to have run pipeline beforehand to get back prediction in native space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib_surface_plotting as msp\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import nibabel as nb\n",
    "import meld_graph.experiment\n",
    "from meld_classifier.paths import BASE_PATH\n",
    "from meld_classifier.meld_cohort import MeldCohort,MeldSubject\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prediction(subject,hdf5,dset='prediction_clustered'):\n",
    "    results={}\n",
    "    with h5py.File(hdf5, \"r\") as f:\n",
    "        for hemi in ['lh','rh']:\n",
    "            results[hemi] = f[subject][hemi][dset][:]\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot salient vertices and saliencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from meld_graph.dataset import GraphDataset\n",
    "from meld_graph.evaluation import Evaluator\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "from meld_graph.data_preprocessing import Preprocess\n",
    "from meld_graph.confidence import get_confidence\n",
    "import matplotlib_surface_plotting as msp\n",
    "from meld_classifier.meld_plotting import trim\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subj_data(subject_id, eva):\n",
    "    \n",
    "    #load data for that subject\n",
    "    data_dictionary = eva.load_data_from_file(subject_id, keys=['result','cluster_thresholded','input_features'], \n",
    "                                          split_hemis=True, )\n",
    "    features_vals = data_dictionary['input_features']\n",
    "    predictions = data_dictionary['cluster_thresholded']\n",
    "\n",
    "    #find thresholds used if two thresholds\n",
    "    if isinstance(eva.threshold, np.ndarray):\n",
    "        if max(data_dictionary['result']['left'].max(), data_dictionary['result']['right'].max()) > eva.threshold[1]:\n",
    "            threshold_text = \"high confidence cluster\"\n",
    "        else :\n",
    "            threshold_text = \"No high confidence cluster\\nLow confidence cluster given instead\"\n",
    "    else:\n",
    "        threshold_text = \"\"\n",
    "    \n",
    "    #find clusters and load saliencies and confidence\n",
    "    list_clust = {}\n",
    "    confidences = {}\n",
    "    saliencies = {}\n",
    "    for hemi in ['left','right']:\n",
    "        list_clust[hemi] = set(predictions[hemi])\n",
    "        list_clust[hemi].remove(0.0)\n",
    "        keys = [f'saliencies_{cl}' for cl in list_clust[hemi]] + [f'mask_salient_{cl}' for cl in list_clust[hemi]]\n",
    "        saliencies.update(eva.load_data_from_file(subject_id, \n",
    "                                            keys=keys, \n",
    "                                            split_hemis=True))\n",
    "    \n",
    "        for cl in list_clust[hemi]:\n",
    "            mask_salient = saliencies[f'mask_salient_{cl}'][hemi].astype(bool)\n",
    "            confidence_cl_salient = np.max(data_dictionary['result'][hemi][mask_salient])\n",
    "            confidences[f'confidence_{cl}'] =  confidence_cl_salient\n",
    "\n",
    "    return list_clust, features_vals, predictions, threshold_text, saliencies, confidences\n",
    "\n",
    "def load_cmap():\n",
    "    \"\"\" create the colors dictionarry for the clusters\"\"\"\n",
    "    from matplotlib.colors import ListedColormap\n",
    "    import numpy as np\n",
    "    colors =  [\n",
    "        [255,0,0],     #red\n",
    "        [255,215,0],   #gold\n",
    "        [0,0,255],     #blue\n",
    "        [0,128,0],     #green\n",
    "    ]\n",
    "    colors=np.array(colors)/255\n",
    "    dict_c = dict(zip(np.arange(1, len(colors)+1), colors))\n",
    "    cmap = ListedColormap(colors)\n",
    "    return cmap, dict_c\n",
    "\n",
    "def create_surface_plots(surf,prediction,c, base_size=20):\n",
    "    \"\"\"plot and reload surface images\"\"\"\n",
    "    cmap, colors =  load_cmap()\n",
    "    tmp_file = 'tmp.png'\n",
    "    msp.plot_surf(surf['coords'],\n",
    "              surf['faces'],prediction,\n",
    "              rotate=[90],\n",
    "              mask=prediction==0,pvals=np.ones_like(c.cortex_mask),\n",
    "              colorbar=False,vmin=1,vmax=len(colors) ,cmap=cmap,\n",
    "              base_size=base_size,\n",
    "              filename=tmp_file)\n",
    "    im = Image.open(tmp_file)\n",
    "    im = trim(im)\n",
    "    im = im.convert(\"RGBA\")\n",
    "    im1 = np.array(im)\n",
    "    msp.plot_surf(surf['coords'],\n",
    "            surf['faces'],prediction,\n",
    "              rotate=[270],\n",
    "              mask=prediction==0,pvals=np.ones_like(c.cortex_mask),\n",
    "              colorbar=False,vmin=1,vmax=len(colors),cmap=cmap,\n",
    "              base_size=base_size,\n",
    "              filename=tmp_file)\n",
    "    im = Image.open(tmp_file)\n",
    "    im = trim(im)\n",
    "    im = im.convert(\"RGBA\")\n",
    "    im2 = np.array(im)\n",
    "    plt.close('all')\n",
    "    os.remove(tmp_file)\n",
    "    return im1,im2\n",
    "\n",
    "def get_key(dic, val):\n",
    "    # function to return key for any value in dictionnary\n",
    "    for key, value in dic.items():\n",
    "        if val == value:\n",
    "            return key\n",
    "    return \"No key for value {}\".format(val)\n",
    "\n",
    "def define_atlas():\n",
    "    file = os.path.join(BASE_PATH, \"fsaverage_sym\", \"label\", \"lh.aparc.annot\")\n",
    "    atlas = nb.freesurfer.io.read_annot(file)\n",
    "    vertex_i = np.array(atlas[0]) - 1000  # subtract 1000 to line up vertex\n",
    "    rois_prop = [\n",
    "        np.count_nonzero(vertex_i == x) for x in set(vertex_i)\n",
    "    ]  # proportion of vertex per rois\n",
    "    rois = [x.decode(\"utf8\") for x in atlas[2]]  # extract rois label from the atlas\n",
    "    rois = dict(zip(rois, range(len(rois))))  # extract rois label from the atlas\n",
    "    rois.pop(\"unknown\")  # roi not part of the cortex\n",
    "    rois.pop(\"corpuscallosum\")  # roi not part of the cortex\n",
    "    return rois, vertex_i, rois_prop\n",
    "\n",
    "def get_cluster_location(cluster_array):\n",
    "    cluster_array = np.array(cluster_array)\n",
    "    rois, vertex_i, rois_prop = define_atlas()\n",
    "    pred_rois = list(vertex_i[cluster_array])\n",
    "    pred_rois = np.array([[x, pred_rois.count(x)] for x in set(pred_rois) if x != 0])\n",
    "    ind = pred_rois[np.where(pred_rois == pred_rois[:,1].max())[0]][0][0]\n",
    "    location = get_key(rois,ind)\n",
    "    return location\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'H101'  # \"\"test\" or \"H101\"\n",
    "\n",
    "# subjects=[\n",
    "#         'MELD_H16_3T_FCD_004',\n",
    "#           # 'MELD_H4_3T_FCD_0011'\n",
    "#         ]\n",
    "\n",
    "subjects=[\n",
    "        #'MELD_H101_3T_FCD_00068' # low confidence one , not used anymore removed\n",
    "        \n",
    "        #  'MELD_H101_3T_FCD_00138', # high confidence, MRI+ve H101\n",
    "        #  'MELD_H101_3T_FCD_00062', #  low confidence, MRI-ve H101\n",
    "\n",
    "         'MELD_H101_3T_FCD_00108', #  high confidence, MRI-ve H101\n",
    "         'MELD_H101_3T_FCD_00121', #  low confidence, MRI-ve H101\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load experiment\n",
    "model_graph = 'experiments_graph/kw350/23-10-30_LVHZ_dcp/s_0/fold_all_newthreshold'\n",
    "exp = meld_graph.experiment.Experiment.from_folder(model_graph)\n",
    "exp.data_parameters[\"augment_data\"] = {}\n",
    "\n",
    "#load trainval dataset\n",
    "split = \"test\"\n",
    "\n",
    "# if load subjects from test \n",
    "if dataset == 'H101':\n",
    "    save_dir = 'experiments_graph/kw350/23-10-30_LVHZ_dcp/s_0/fold_all_newthreshold/test_H27H28H101'\n",
    "    cohort = MeldCohort(\n",
    "            hdf5_file_root=\"{site_code}_{group}_featurematrix_combat_freesurfer_harmonised_NewSite.hdf5\",\n",
    "            dataset='MELD_dataset_NewSiteH27H28H101_freesurfer.csv',\n",
    "    )\n",
    "else:\n",
    "    save_dir=None\n",
    "    cohort = MeldCohort(\n",
    "        hdf5_file_root=exp.data_parameters[\"hdf5_file_root\"],\n",
    "        dataset=exp.data_parameters[\"dataset\"],\n",
    "    )\n",
    "\n",
    "\n",
    "features = exp.data_parameters[\"features\"]\n",
    "\n",
    "dataset = GraphDataset(subjects, cohort, exp.data_parameters, mode=\"test\")\n",
    "save_prediction_suffix=\"\"\n",
    "\n",
    "# create evaluator\n",
    "eva = Evaluator(\n",
    "    experiment=exp,\n",
    "    checkpoint_path=model_graph,\n",
    "    make_images=True,\n",
    "    dataset=dataset,\n",
    "    save_dir=save_dir,\n",
    "    cohort=cohort,\n",
    "    subject_ids=subjects,\n",
    "    mode=\"test\",\n",
    "    thresh_and_clust=True,\n",
    "    threshold='slope_threshold',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot prediction and lesion\n",
    "# eva.plot_subjects_prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # calculate saliencies if does not exists\n",
    "# eva.calculate_saliency(save_prediction_suffix=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'meld_data/Example_pts_graph/input'\n",
    "output_dir = 'meld_data/Example_pts_graph/output'\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing test data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MELD_H101_3T_FCD_00108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Z-scoring data for MELD_H101_3T_FCD_00108\n",
      "Loading and preprocessing test data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MELD_H101_3T_FCD_00121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Z-scoring data for MELD_H101_3T_FCD_00121\n"
     ]
    }
   ],
   "source": [
    "\n",
    " # setup parameters\n",
    "base_feature_sets = [\n",
    "    \".on_lh.gm_FLAIR_0.5.sm3.mgh\",\n",
    "    \".on_lh.wm_FLAIR_1.sm3.mgh\",\n",
    "    \".on_lh.curv.sm3.mgh\",\n",
    "    \".on_lh.pial.K_filtered.sm20.mgh\",\n",
    "    \".on_lh.sulc.sm3.mgh\",\n",
    "    \".on_lh.thickness_regression.sm3.mgh\",\n",
    "    \".on_lh.w-g.pct.sm3.mgh\",\n",
    "]\n",
    "\n",
    "feature_names_sets = [\n",
    "    \"GM FLAIR (50%)\",\n",
    "    \"WM FLAIR (1mm)\",\n",
    "    \"Mean curvature\",\n",
    "    \"Intrinsic Curvature\",\n",
    "    \"Sulcal depth\",\n",
    "    \"Cortical thickness\",\n",
    "    \"Grey-white contrast\",\n",
    "]\n",
    "NVERT=293804\n",
    "\n",
    "for subject_id in subjects:\n",
    "    subject = MeldSubject(subject_id, cohort=cohort)\n",
    "\n",
    "    #create results folder\n",
    "    os.makedirs(os.path.join(output_dir,subject_id,'reports'), exist_ok=True)\n",
    "    # initialise parameter for plot\n",
    "    fig = plt.figure(figsize=(15, 8), constrained_layout=True)\n",
    "    if subject.has_flair:\n",
    "        base_features = base_feature_sets\n",
    "        feature_names = feature_names_sets\n",
    "    else:\n",
    "        base_features = base_feature_sets[2:]\n",
    "        feature_names = feature_names_sets[2:]\n",
    "    # load predictions and data subject\n",
    "    list_clust, features_vals, predictions, threshold_text, saliencies, confidences  = get_subj_data(subject_id, eva)\n",
    "    # Loop over hemi\n",
    "    for i, hemi in enumerate([\"left\", \"right\"]):\n",
    "        # prepare grid plot\n",
    "        gs1 = GridSpec(2, 3, width_ratios=[1, 1, 1], wspace=0.1, hspace=0.1)\n",
    "        gs2 = GridSpec(2, 4, height_ratios=[1, 3], width_ratios=[1, 1, 0.5, 2], wspace=0.1)\n",
    "        gs3 = GridSpec(1, 1)\n",
    "        # plot predictions on inflated brain\n",
    "        im1, im2 = create_surface_plots(cohort.surf, prediction=predictions[hemi], c=cohort)\n",
    "        if hemi == \"right\":\n",
    "            im1 = im1[:, ::-1]\n",
    "            im2 = im2[:, ::-1]\n",
    "        ax = fig.add_subplot(gs1[i, 1])\n",
    "        ax.imshow(im1)\n",
    "        ax.axis(\"off\")\n",
    "        title = 'Left hemisphere' if hemi=='left' else 'Right hemisphere'\n",
    "        ax.set_title(title, loc=\"left\", fontsize=20)\n",
    "        ax = fig.add_subplot(gs1[i, 2])\n",
    "        ax.imshow(im2)\n",
    "        ax.axis(\"off\")\n",
    "        # initiate params for saliencies\n",
    "        prefixes = [\".combat\", \".inter_z.intra_z.combat\", \".inter_z.asym.intra_z.combat\"]\n",
    "        cmap = mpl.colors.LinearSegmentedColormap.from_list(\n",
    "            \"grpr\",\n",
    "            colors=[\n",
    "                \"#276419\",\n",
    "                \"#FFFFFF\",\n",
    "                \"#8E0152\",\n",
    "            ],\n",
    "        )\n",
    "        labels = [\"Harmonised\", \"Normalised\", \"Asymmetry\"]\n",
    "        hatching = [\"\\\\\\\\\", \"//\", \"--\"]\n",
    "        # loop over clusters\n",
    "        for cluster in list_clust[hemi]:\n",
    "            fig2 = plt.figure(figsize=(17, 9))\n",
    "            # get and plot saliencies\n",
    "            saliencies_cl = saliencies[f'saliencies_{cluster}'][hemi]\n",
    "            saliencies_cl = saliencies_cl * (NVERT/2)\n",
    "            # plot prediction and salient vertices\n",
    "            mask = np.array([predictions[hemi] == cluster])[0]\n",
    "            mask_salient = saliencies[f'mask_salient_{cluster}'][hemi].astype(bool)\n",
    "            mask_comb = mask.astype(int)+mask_salient.astype(int)                \n",
    "            lims_saliencies_cl = 1.1*np.max([np.max(np.mean(saliencies_cl[mask_salient], axis=0)),-np.min(np.mean(saliencies_cl[mask_salient], axis=0))])\n",
    "            norm = mpl.colors.Normalize(vmin=-lims_saliencies_cl, vmax=lims_saliencies_cl)\n",
    "            m = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "            \n",
    "            im1, im2 = create_surface_plots(cohort.surf, prediction=mask_comb, c=cohort, base_size=10)\n",
    "            if hemi == \"right\":\n",
    "                im1 = im1[:, ::-1]\n",
    "                im2 = im2[:, ::-1]\n",
    "            ax2 = fig2.add_subplot(gs2[1, 0])\n",
    "            ax2.imshow(im1)\n",
    "            ax2.axis(\"off\")                \n",
    "            ax2 = fig2.add_subplot(gs2[1, 1])\n",
    "            ax2.imshow(im2)\n",
    "            ax2.axis(\"off\")\n",
    "            # ax2.set_title('Predicted cluster and\\n20% most salient vertices', loc=\"left\", fontsize=15)\n",
    "            ax2 = fig2.add_subplot(gs2[:, 3],)\n",
    "            for pr, prefix in enumerate(prefixes):\n",
    "                cur_data = np.zeros(len(base_features))\n",
    "                cur_err = np.zeros(len(base_features))\n",
    "                saliency_data = np.zeros(len(base_features))\n",
    "                for b, bf in enumerate(base_features):\n",
    "                    cur_data[b] = np.mean(\n",
    "                        np.array(features_vals[hemi][mask_salient, features.index(prefix + bf)])\n",
    "                    )\n",
    "                    cur_err[b] = np.std(\n",
    "                        np.array(features_vals[hemi][mask_salient, features.index(prefix + bf)])\n",
    "                    )\n",
    "                    saliency_data[b] = np.mean(\n",
    "                        saliencies_cl[mask_salient ,features.index(prefix + bf)]\n",
    "                        )\n",
    "                ax2.barh(\n",
    "                    y=np.array(range(len(base_features))) - pr * 0.3,\n",
    "                    width=cur_data,\n",
    "                    hatch=hatching[pr],\n",
    "                    height=0.3,\n",
    "                    edgecolor=\"k\",\n",
    "                    xerr=cur_err,\n",
    "                    label=labels[pr],\n",
    "                    color=m.to_rgba(saliency_data),\n",
    "                )\n",
    "            limvals = np.max([np.max(cur_data+cur_err),-np.min(cur_data-cur_err)])+0.5\n",
    "            ax2.set_xlim([-limvals, limvals])\n",
    "            # ax2.set_xticks([])\n",
    "            ax2.set_yticks(np.array(range(len(base_features))) - 0.23)\n",
    "            ax2.set_yticklabels(feature_names, fontsize=16)\n",
    "            ax2.set_xlabel(\"Z score\", fontsize=16)\n",
    "            ax2.legend(loc=\"upper center\", bbox_to_anchor=(0.5, 1.17), fontsize=16)\n",
    "            fig2.colorbar(m, ax=ax2).set_label(label='Saliency',size=18,weight='bold')\n",
    "            ax2.set_autoscale_on(True)\n",
    "            ## display info cluster\n",
    "            # get size\n",
    "            size_clust = np.sum(cohort.surf_area[predictions[hemi] == cluster]) / 100\n",
    "            size_clust = round(size_clust, 3)\n",
    "            # get location\n",
    "            location = get_cluster_location(predictions[hemi] == cluster)\n",
    "            # get confidence\n",
    "            confidence = round(confidences[f'confidence_{cluster}'].mean(),2)\n",
    "            # plot info in text box in upper left in axes coords\n",
    "            textstr = \"\\n\".join(\n",
    "                (\n",
    "                    f\" Cluster {int(cluster)} on the {hemi} hemisphere\",\n",
    "                    \" \",\n",
    "                    f\" Cluster size = {size_clust} cm2\",\n",
    "                    \" \",\n",
    "                    f\" Cortical region =  {location}\",\n",
    "                        \" \",\n",
    "                    f\" Confidence score =  {confidence}\",\n",
    "                    \" \",\n",
    "                    f\"-{threshold_text}\",\n",
    "                    \n",
    "                    \n",
    "                )\n",
    "            )\n",
    "            props = dict(boxstyle=\"round\", alpha=0.5)\n",
    "            ax2 = fig2.add_subplot(gs2[0, 0:2])\n",
    "            ax2.text(0.05, 0.95, textstr, transform=ax2.transAxes, fontsize=18, verticalalignment=\"top\", bbox=props)\n",
    "            ax2.axis(\"off\")\n",
    "            fig2.savefig(f\"{output_dir}/{subject_id}/reports/saliency_{subject.subject_id}_{hemi}_c{int(cluster)}_combat.png\", facecolor='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot MRI\n",
    "import glob\n",
    "from nilearn import plotting, image\n",
    "from nilearn.image import new_img_like\n",
    "from nilearn._utils.numpy_conversions import as_ndarray\n",
    "\n",
    "subject_id= 'MELD_H101_3T_FCD_00121'\n",
    "\n",
    "# Open their MRI data if available\n",
    "t1_file = glob.glob(os.path.join(input_dir,subject_id,'T1', '*.nii.gz'))[0]\n",
    "prediction_file = glob.glob(os.path.join(output_dir, subject_id, \"predictions\", \"prediction*\"))[0]\n",
    "# load image\n",
    "imgs = {\n",
    "    \"anat\": nb.load(t1_file),\n",
    "    \"pred\": nb.load(prediction_file),\n",
    "}\n",
    "# # Resample and move to same shape and affine than t1\n",
    "imgs[\"pred\"] = image.resample_img(\n",
    "    imgs[\"pred\"],\n",
    "    target_affine=imgs[\"anat\"].affine,\n",
    "    target_shape=imgs[\"anat\"].shape,\n",
    "    interpolation=\"nearest\",\n",
    "    copy=True,\n",
    "    order=\"F\",\n",
    "    clip=False,\n",
    "    fill_value=0,\n",
    "    force_resample=False,\n",
    ")\n",
    "# initialise parameter for plot\n",
    "fig = plt.figure(figsize=(15, 8), constrained_layout=True)\n",
    "# Loop over hemi\n",
    "for i, hemi in enumerate([\"left\", \"right\"]):\n",
    "    # prepare grid plot\n",
    "    gs3 = GridSpec(1, 1)\n",
    "    for cluster in list_clust[hemi]:\n",
    "        # plot cluster on anat MRI volume\n",
    "        fig3 = plt.figure(figsize=(15, 8))\n",
    "        ax3 = fig3.add_subplot(gs3[0])\n",
    "        min_v = cluster - 1\n",
    "        max_v = cluster + 1\n",
    "        mask = image.math_img(f\"(img < {max_v}) & (img > {min_v})\", img=imgs[f\"pred\"])\n",
    "        coords = plotting.find_xyz_cut_coords(mask)\n",
    "        vmax = np.percentile(imgs[\"anat\"].get_fdata(), 99)\n",
    "        display = plotting.plot_anat(\n",
    "                        t1_file, colorbar=False, cut_coords=coords, \n",
    "                        draw_cross=False, radiological=True,\n",
    "                        figure=fig3, axes=ax3, vmax=vmax\n",
    "                    )        \n",
    "        for cut_ax in display.axes.values():\n",
    "                    slices_x = np.linspace(cut_ax.ax.get_xlim()[0], cut_ax.ax.get_xlim()[1],100)\n",
    "                    cut_ax.ax.set_xlim(slices_x[12], slices_x[-12])\n",
    "                    slices_y = np.linspace(cut_ax.ax.get_ylim()[0], cut_ax.ax.get_ylim()[1],100)\n",
    "                    cut_ax.ax.set_ylim(slices_y[12], slices_y[-12])\n",
    "        fig3.savefig(f\"{output_dir}/{subject_id}/reports/mri_{subject_id}_{hemi}_c{int(cluster)}_rawT1.png\")     \n",
    "        #display cluster\n",
    "        data = imgs[\"pred\"].get_fdata()\n",
    "        map_img = new_img_like(imgs[\"pred\"], as_ndarray((data==cluster) | (data==cluster*100)).astype(float), imgs[\"pred\"].affine)\n",
    "        display.add_contours(\n",
    "                map_img,\n",
    "                levels=[0.5],\n",
    "                colors=[\"red\"],\n",
    "                filled=True,\n",
    "                alpha=0.7,\n",
    "                linestyles=\"solid\",\n",
    "            )\n",
    "        # display cluster salient vertices\n",
    "        map_img = new_img_like(imgs[\"pred\"], as_ndarray(data==cluster*100).astype(float), imgs[\"pred\"].affine)\n",
    "        display.add_contours(\n",
    "                map_img,\n",
    "                levels=[0.5],\n",
    "                colors=[\"yellow\"],\n",
    "                filled=True,\n",
    "                alpha=0.7,\n",
    "                linestyles=\"solid\",\n",
    "            )\n",
    "        fig3.savefig(f\"{output_dir}/{subject_id}/reports/mri_{subject_id}_{hemi}_c{int(cluster)}_T1pred.png\")\n",
    "\n",
    "        # display FLAIR if exists\n",
    "        flair_file = glob.glob(os.path.join(input_dir,subject_id,'FLAIR', '*.nii.gz'))[0]\n",
    "        if os.path.isfile(flair_file):\n",
    "            fig4 = plt.figure(figsize=(15, 8))\n",
    "            ax4 = fig4.add_subplot(gs3[0])\n",
    "            vmax = np.percentile(nb.load(flair_file).get_fdata(), 99)\n",
    "            display2 = plotting.plot_anat(\n",
    "                            flair_file, colorbar=False, cut_coords=coords, \n",
    "                            draw_cross=False, radiological=True,\n",
    "                            figure=fig4, axes=ax4, vmax=vmax\n",
    "                        )\n",
    "            for cut_ax in display2.axes.values():\n",
    "                    slices_x = np.linspace(cut_ax.ax.get_xlim()[0], cut_ax.ax.get_xlim()[1],100)\n",
    "                    cut_ax.ax.set_xlim(slices_x[12], slices_x[-12])\n",
    "                    slices_y = np.linspace(cut_ax.ax.get_ylim()[0], cut_ax.ax.get_ylim()[1],100)\n",
    "                    cut_ax.ax.set_ylim(slices_y[12], slices_y[-12])\n",
    "            fig4.savefig(f\"{output_dir}/{subject_id}/reports/mri_{subject_id}_{hemi}_c{int(cluster)}_raw_FLAIR.png\")     \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "26e275ceef4766aa0e798dd77381c3d2e5e3e504ec96375b0267e7506358c3a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

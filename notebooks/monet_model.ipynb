{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "install pytorch-geometric\n",
    "on mac: using pip wheel installation instructions, with python 3.9 and pytorch via conda (https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html)\n",
    "\n",
    "loading data: \n",
    "ultimately: build on-disk dataset w/ pytorch \n",
    "atm: build dataset on the fly with synthetic dataset + data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# coords used in segmentation paper (polar is 2d, how?)\n",
    "#relative_polar = np.load('/Users/hannah.spitzer/projects/MELD/Benchmarking-Surface-DL/Segmentation_UGSCNN/GraphMethods/data/relative_coords_polar_3.npy')\n",
    "#relative_coord = np.load('/Users/hannah.spitzer/projects/MELD/Benchmarking-Surface-DL/Segmentation_UGSCNN/GraphMethods/data/relative_coords_3.npy')\n",
    "#edge_index_seg = np.load('/Users/hannah.spitzer/projects/MELD/Benchmarking-Surface-DL/Segmentation_UGSCNN/GraphMethods/data/edge_index_3.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.transforms import Cartesian, Polar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fake_edge_index(level=1):\n",
    "    # TODO define this for multiple levels for the real data\n",
    "    edge_index = torch.tensor([[0, 1],\n",
    "                            [1, 0],\n",
    "                            [1, 2],\n",
    "                            [2, 1],[0,0],[1,1],[2,2]], dtype=torch.long)\n",
    "    # get relative cartensian / polar coords as edge_attrs to use a pseudo coordinates later on\n",
    "    position = torch.tensor([[0,0],[1,1],[2,0]], dtype=torch.float)\n",
    "    data = Data(edge_index=edge_index.t().contiguous(), pos=position)\n",
    "    edge_attr_polar = Polar(norm=True, cat=False)(data).edge_attr  # TODO figure out how exactly want to do coord transform (should be relative difference between nodes)\n",
    "    return edge_index.t().contiguous(), edge_attr_polar\n",
    "\n",
    "def get_fake_data():\n",
    "    # fake some patient data with 3 nodes and per-node labels, and 5 features\n",
    "    # build data list\n",
    "    data_list = []\n",
    "    for i in range(100):\n",
    "        x = torch.tensor(np.random.rand(3,5), dtype=torch.float)\n",
    "        y = torch.tensor(np.random.choice([0,1], size=(3,1)))\n",
    "        data_list.append(Data(x=x, y=y))\n",
    "\n",
    "    loader = DataLoader(data_list, batch_size=2)\n",
    "\n",
    "    # can put loader + model on GPU here\n",
    "    # could use transforms on data loader here\n",
    "    # example loading of data + custom dataset for brain data: https://github.com/Abdulah-Fawaz/Benchmarking-Surface-DL/blob/main/Segmentation_UGSCNN/GraphMethods/segmentation.py\n",
    "\n",
    "    return loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n",
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n",
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n",
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n",
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n",
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n",
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n",
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n",
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n",
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n",
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n",
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n",
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n",
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n",
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n",
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n",
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n",
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n",
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n",
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n",
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n",
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n",
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n",
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n",
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n",
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n",
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n",
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n",
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n",
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n",
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n",
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n",
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n",
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n",
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n",
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n",
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n",
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n",
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n",
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n",
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n",
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n",
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n",
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n",
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n",
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n",
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n",
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n",
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n",
      "DataBatch(x=[6, 5], edge_index=[2, 14], edge_attr=[14, 2], y=[6, 1], batch=[6], ptr=[3])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for batch in loader:\n",
    "    print(batch)\n",
    "    print(batch.num_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import GMMConv\n",
    "\n",
    "# define model\n",
    "class MoNet(torch.nn.Module):\n",
    "    def __init__(self, num_features, layer_sizes, dim=2, kernel_size=3, edge_index=get_fake_edge_index):\n",
    "        \"\"\"\n",
    "        dim: dim for GMMConv, dimension of coord representation - 2 or 3\n",
    "        kernel_size: number of kernels (default 3)\n",
    "        layer_sizes: (list) output size of each conv layer. a final linear layer for going to 2 (binary classification) is added\n",
    "        num_features: number of input features (input size)\n",
    "        \"\"\"\n",
    "        super(MoNet, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        assert len(layer_sizes) >= 1\n",
    "        layer_sizes.insert(0, num_features)\n",
    "        self.layer_sizes = layer_sizes\n",
    "        conv_layers = []\n",
    "        for in_size, out_size in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "            conv_layers.append(GMMConv(in_size, out_size, dim=dim, kernel_size=kernel_size))\n",
    "        self.conv_layers = torch.nn.Sequential(*conv_layers)\n",
    "        self.fc = torch.nn.Linear(self.layer_sizes[-1], 2)\n",
    "        self.activation_function = torch.nn.ReLU()\n",
    "        self.edge_indices = [edge_index(level=1)[0]]\n",
    "        self.edge_attrs = [edge_index(level=1)[1]]\n",
    "\n",
    "        # initialisation - TODO figure out what we want\n",
    "        # in segmentation benchmark they don't do any -- use default\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data\n",
    "        for cl in self.conv_layers:\n",
    "            x = cl(x, self.edge_indices[0], self.edge_attrs[0])\n",
    "            x = self.activation_function(x)\n",
    "        # add final linear layer\n",
    "        x = self.activation_function(self.fc(x))\n",
    "        return torch.nn.LogSoftmax(dim=1)(x)\n",
    "\n",
    "\n",
    "model = MoNet(num_features=5, layer_sizes=[16,10], dim=2, edge_index=get_fake_edge_index)\n",
    "# TODO try and fit model\n",
    "# think about initialisatiomn\n",
    "# coord rep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "import logging\n",
    "\n",
    "network_parameters = {\n",
    "    'network_type': 'MoNet',\n",
    "    'model_parameters': {\n",
    "        'layer_sizes': [16,10],\n",
    "        'dim': 2, # pseudo-coord dim\n",
    "        'num_features': 5 # number of input features\n",
    "    },\n",
    "    \"max_patience\": 10,\n",
    "    \"num_epochs\": 100,\n",
    "    'lr': 1e-4\n",
    "}\n",
    "\n",
    "class Experiment:\n",
    "    def __init__(self, network_parameters):\n",
    "        self.network_parameters = network_parameters\n",
    "        # TODO save parameters\n",
    "\n",
    "        self.model = None # loaded by self.load_model()\n",
    "        self.log = logging.getLogger(__name__)\n",
    "\n",
    "    def from_folder(self, path):\n",
    "        pass\n",
    "\n",
    "    def load_model(self, checkpoint_path=None):\n",
    "        \"\"\"\n",
    "        build model and optionally load weights from checkpoint\n",
    "        \"\"\"\n",
    "        # build model using network_parameters\n",
    "        network_type = self.network_parameters['network_type']\n",
    "        if network_type == 'MoNet':\n",
    "            self.model = MoNet(**self.network_parameters['model_parameters'], edge_index=get_fake_edge_index)\n",
    "        else:\n",
    "            raise(NotImplementedError, network_type)\n",
    "\n",
    "        # load model weights\n",
    "\n",
    "        #torch.save(model.state_dict(), PATH)\n",
    "\n",
    "        if checkpoint_path is not None and os.path.isdir(checkpoint_path):\n",
    "            # checkpoint contains both model architecture + weights\n",
    "            self.log.info(\"Loading model weights from checkpoint\")\n",
    "            self.model.load_state_dict(torch.load(checkpoint_path))\n",
    "            self.model.eval()\n",
    "\n",
    "    def train(self, save_path=None):\n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "        # set up model\n",
    "        if self.model is None:\n",
    "            self.load_model()\n",
    "        self.model.to(device)\n",
    "\n",
    "        # get data\n",
    "        train_data_loader = get_fake_data()\n",
    "        val_data_loader = get_fake_data()\n",
    "        # get edge_\n",
    "\n",
    "        # set up training loop\n",
    "        optimiser = torch.optim.Adam(self.model.parameters(),lr=self.network_parameters['lr'])\n",
    "    \n",
    "        validation_losses = []\n",
    "        train_losses = []\n",
    "    \n",
    "        best_loss = 100000\n",
    "        patience = 0\n",
    "        for epoch in range(self.network_parameters['num_epochs']):\n",
    "            running_losses = []\n",
    "            \n",
    "            for i, data in enumerate(train_data_loader):  \n",
    "                data = data.to(device)\n",
    "                self.model.train()     \n",
    "                optimiser.zero_grad()\n",
    "                estimates = self.model(data.x)\n",
    "                labels = data.y.squeeze()\n",
    "                loss = torch.nn.NLLLoss()(estimates, labels)\n",
    "                loss.backward()\n",
    "                optimiser.step()\n",
    "                running_losses.append(loss.item())\n",
    "          \n",
    "            print('Epoch {} :: Train loss {:.3f}'.format(epoch,np.mean(running_losses)))\n",
    "            train_losses.append(np.mean(running_losses))\n",
    "        \n",
    "            if epoch%1 ==0:\n",
    "                with torch.no_grad():\n",
    "                    running_losses  = []\n",
    "                    for i, data in enumerate(val_data_loader):\n",
    "                        data = data.to(device)\n",
    "                        estimates = self.model(data.x)\n",
    "                        labels = data.y.squeeze()\n",
    "                        loss = torch.nn.NLLLoss()(estimates, labels)\n",
    "                        running_losses.append(loss.item())\n",
    "                            \n",
    "                    val_loss = np.mean(running_losses)\n",
    "                    validation_losses.append(val_loss)\n",
    "                    print('validation ', val_loss)\n",
    "                        \n",
    "                    if val_loss < best_loss:\n",
    "                        best_loss = val_loss\n",
    "                        if save_path is not None:\n",
    "                            torch.save(self.model.state_dict(), save_path)\n",
    "                            print('saved_new_best')\n",
    "                        patience = 0\n",
    "                    else:\n",
    "                        patience+=1\n",
    "                    if patience >= self.network_parameters['max_patience']:\n",
    "                        break\n",
    "                    \n",
    "                    print('----------')\n",
    "        # save final model\n",
    "        #torch.save(model,'/home/lw19/Desktop/final_benchmarking_models/final_GConvNet_TopK_Native_warp_no_rot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment(network_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 :: Train loss 0.693\n",
      "validation  0.6931207931041717\n",
      "----------\n",
      "Epoch 1 :: Train loss 0.693\n",
      "validation  0.6931080949306488\n",
      "----------\n",
      "Epoch 2 :: Train loss 0.693\n",
      "validation  0.693099069595337\n",
      "----------\n",
      "Epoch 3 :: Train loss 0.693\n",
      "validation  0.693090192079544\n",
      "----------\n",
      "Epoch 4 :: Train loss 0.693\n",
      "validation  0.6930792272090912\n",
      "----------\n",
      "Epoch 5 :: Train loss 0.693\n",
      "validation  0.6930769991874695\n",
      "----------\n",
      "Epoch 6 :: Train loss 0.693\n",
      "validation  0.6930726075172424\n",
      "----------\n",
      "Epoch 7 :: Train loss 0.693\n",
      "validation  0.6930650472640991\n",
      "----------\n",
      "Epoch 8 :: Train loss 0.692\n",
      "validation  0.6930631458759308\n",
      "----------\n",
      "Epoch 9 :: Train loss 0.692\n",
      "validation  0.6930594158172607\n",
      "----------\n",
      "Epoch 10 :: Train loss 0.692\n",
      "validation  0.6930659806728363\n",
      "----------\n",
      "Epoch 11 :: Train loss 0.692\n",
      "validation  0.6930728471279144\n",
      "----------\n",
      "Epoch 12 :: Train loss 0.692\n",
      "validation  0.6930850327014924\n",
      "----------\n",
      "Epoch 13 :: Train loss 0.692\n",
      "validation  0.6930976104736328\n",
      "----------\n",
      "Epoch 14 :: Train loss 0.692\n",
      "validation  0.6931075584888459\n",
      "----------\n",
      "Epoch 15 :: Train loss 0.692\n",
      "validation  0.6931077253818512\n",
      "----------\n",
      "Epoch 16 :: Train loss 0.692\n",
      "validation  0.6931039369106293\n",
      "----------\n",
      "Epoch 17 :: Train loss 0.692\n",
      "validation  0.6930904614925385\n",
      "----------\n",
      "Epoch 18 :: Train loss 0.692\n",
      "validation  0.6930774140357971\n",
      "----------\n",
      "Epoch 19 :: Train loss 0.692\n",
      "validation  0.693060758113861\n"
     ]
    }
   ],
   "source": [
    "exp.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e336a35d61f60f865ab0873d0d0cc82dcfe64a783bf2c2b281a1493d54951c12"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('meld-graph')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
